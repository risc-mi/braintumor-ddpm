{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1791d22c",
   "metadata": {},
   "source": [
    "## Dataset Extraction and Data Splits\n",
    "\n",
    "In this notebook, we create the 2D dataset from the original BraTS 3D volumes. This is done by exporting a selection of axial slices from each volume across the four different modalities in BraTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf79c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Add braintumor_ddpm to path\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# braintumor_ddpm imports\n",
    "from braintumor_ddpm.data.brats import BRATS\n",
    "from braintumor_ddpm.data.datasets import SegmentationDataset\n",
    "from braintumor_ddpm.utils.convert_data import convert_brats_to_2d, move_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa55711",
   "metadata": {},
   "source": [
    "## 1. Dataset Extraction\n",
    "\n",
    "In our experiments we extract data to two different formats. The first one is to TIFF-based format and the other is to NIfTI format. We show how to export data in the below sections accordingly. \n",
    "\n",
    "### 1.1. Extraction to TIFF-based files\n",
    "\n",
    "We train our diffusion model, pixel-level classifiers and the retraining of the noise predictor network on multi-page TIFF files, where each page corresponds to a different modality. Extraction can be done by simply calling the `export_stack()` function from the `BRATS` class. By setting `slices` parameter we can choose a list of slices to export, when not specified it defaults to the one we use in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c03049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a 2D dataset out of original 3D volumes\n",
    "output_directory = r\"REPLACE WITH PATH POINTING TO OUTPUT DIRECTORY\"\n",
    "path = r\"REPLACE WITH PATH POINTING TO BRATS TRAINING DATA\"\n",
    "\n",
    "# Extract 2D data from the original 3D dataset\n",
    "brats_dataset = BRATS(path=path)\n",
    "brats_dataset.export_stack(output_path=output_directory, slices=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6fafa",
   "metadata": {},
   "source": [
    "### 1.2. Extraction to NIfTI-based files\n",
    "\n",
    "Because we also compare against a supervised baseline, which is nnUNet V1, we also have to export the same slices in a 2D format compatiable with nnUNet V1. Thus, we export axial slices to NIfTI suitable for training the baseline. Running the below cell, we export the entire dataset by calling the `convert_brats_to_2d()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnUNet compatible dataset extraction\n",
    "nnunet_data_dir = os.path.join(output_directory, \"nnUNet_raw\")\n",
    "convert_brats_to_2d(dataset_path=path,\n",
    "                    target_dir=nnunet_data_dir,\n",
    "                    slices=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06e3d1",
   "metadata": {},
   "source": [
    "### Data Splits\n",
    "\n",
    "The data splits for the down-stream tasks are automatically configured within the `braintumor-ddpm` pipeline. However, the following code block is mainly towards extracting the same data split that is compatible with nnUNet V1. Additionally, we also export the split metadata for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify extracted 2D data paths \n",
    "images_path = os.path.join(output_directory, \"Stacked 2D BRATS Data\", \"scans\")\n",
    "masks_path = os.path.join(output_directory, \"Stacked 2D BRATS Data\", \"masks\")\n",
    "\n",
    "# Create a segmentation dataset object\n",
    "dataset = SegmentationDataset(images_dir=images_path,\n",
    "                              masks_dir=masks_path,\n",
    "                              image_size=128,\n",
    "                              device='cpu',\n",
    "                              verbose=False)\n",
    "\n",
    "# Our main split, with the same seed in our experiments\n",
    "train_pool, test = random_split(dataset=dataset,\n",
    "                                lengths=[757, 8000],\n",
    "                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f\"Training pool: {len(train_pool)} images, Test data: {len(test)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1de5ab",
   "metadata": {},
   "source": [
    "Uncomment the few lines below to extract the data used for the upper-bound supervised model and also the test set of 8000 scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94b7ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exports the same splits for nnUNet comparison. rest of training pool is considered as validation\n",
    "all_seeds = [16, 42, 88, 128, 256]\n",
    "train_images = [10, 20, 30, 40, 50]\n",
    "task_id = 600\n",
    "export_test = False\n",
    "\n",
    "# Uncomment below to export upper-bound as well as test set\n",
    "# all_seeds = [42]\n",
    "# train_images = [757]\n",
    "# task_id = 700\n",
    "# export_test = True\n",
    "\n",
    "output_folder = os.path.join(nnunet_data_dir, \"split_metadata\")\n",
    "data_folder = nnunet_data_dir\n",
    "\n",
    "for train_size in tqdm(train_images):\n",
    "    for seed in all_seeds:\n",
    "        \n",
    "        # Split to acquire training data\n",
    "        train, valid = random_split(dataset=train_pool, lengths=[train_size, 757 - train_size],\n",
    "                                    generator=torch.Generator().manual_seed(seed))\n",
    "        if export_test:\n",
    "            valid = test\n",
    "        \n",
    "        # Create training and validation splits\n",
    "        data_split = {'training': [], 'testing': []}\n",
    "        for i in train.indices:\n",
    "            filename = os.path.basename(dataset.dataset[i]['mask'])\n",
    "            filename = filename.split('_')\n",
    "            slice_id = int(filename[-1].split('.')[0])\n",
    "            filename = f\"BraTS_{int(filename[1]):05d}s{slice_id:03d}\"\n",
    "            data_split['training'].append(filename)\n",
    "\n",
    "        for i in valid.indices:\n",
    "            filename = os.path.basename(dataset.dataset[i]['mask'])\n",
    "            filename = filename.split('_')\n",
    "            slice_id = int(filename[-1].split('.')[0])\n",
    "            filename = f\"BraTS_{int(filename[1]):05d}s{slice_id:03d}\"\n",
    "            data_split['testing'].append(filename)\n",
    "        \n",
    "        # Create Split folder and save split metadata\n",
    "        split_folder = os.path.join(output_folder, f\"{train_size}_samples\")\n",
    "        os.makedirs(split_folder, exist_ok=True)\n",
    "        name = os.path.join(split_folder, f'data_split_size_{train_size}_seed_{seed}.json')\n",
    "        with open(name, 'w') as jf:\n",
    "            json.dump(data_split, jf)\n",
    "        jf.close()\n",
    "        \n",
    "        # Move data to data folder\n",
    "        move_data(target_dir=data_folder,\n",
    "         split=data_split,\n",
    "         images_dir=os.path.join(nnunet_data_dir, \"all_images\"),\n",
    "         labels_dir=os.path.join(nnunet_data_dir, \"all_labels\"),\n",
    "         seed=f\"_{seed}_{train_size}_samples\",\n",
    "         task_id=task_id)\n",
    "        task_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
